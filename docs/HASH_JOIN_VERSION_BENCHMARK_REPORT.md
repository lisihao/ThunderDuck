# Hash Join 版本对比基准测试报告

> **版本**: 1.0 | **日期**: 2026-01-26 | **平台**: Apple Silicon M4

## 一、测试概述

### SQL 语义
```sql
SELECT COUNT(*) FROM build b JOIN probe p ON b.key = p.key
```

### 测试配置
| 配置项 | 详情 |
|-------|------|
| 硬件平台 | Apple Silicon M4 (ARM64 + Neon SIMD + UMA) |
| 理论内存带宽 | 400 GB/s |
| 预热次数 | 3 |
| 测量次数 | 5 |

### 测试版本
| 版本 | 描述 | 核心算法 |
|------|------|---------|
| **v3** | CPU Neon SIMD | Radix 分区 + Neon 批量探测 |
| **v5 两阶段** | 消除重分配 | 计数遍历 → 一次性分配 → 填充 |
| **v5 并行** | 多线程探测 | 4 线程并行分区探测 |
| **v6 链式** | 链式哈希表 | 所有重复键连续存储 |
| **v6 分块** | 分块输出 | 预分配 chunk 减少重分配 |
| **GPU-UMA** | Metal GPU | UMA 零拷贝 + 原子操作收集 |
| **DuckDB** | 基线模拟 | std::sort + 二分查找 |

---

## 二、测试结果

### 2.1 小规模测试 (10K×100K)

| 测试场景 | v3 时间 | v5 两阶段 | v5 并行 | v6 链式 | v6 分块 | DuckDB |
|---------|---------|-----------|---------|---------|---------|--------|
| **全匹配** | **84μs** | 1211μs (14x慢) | 1145μs (14x慢) | 504μs (6x慢) | 290μs (3x慢) | 5220μs |
| **50%匹配** | **128μs** | 1021μs (8x慢) | 1013μs (8x慢) | 700μs (5x慢) | 678μs (5x慢) | 3479μs |
| **10%匹配** | **43μs** | 633μs (15x慢) | 765μs (18x慢) | 416μs (10x慢) | 416μs (10x慢) | 1836μs |

**发现**: v3 在小规模数据上完胜所有版本

### 2.2 中规模测试 (100K×1M)

| 测试场景 | v3 时间 | v5 两阶段 | v6 链式 | GPU-UMA | DuckDB | v3 vs DuckDB |
|---------|---------|-----------|---------|---------|--------|--------------|
| **全匹配** | **931μs** | 5114μs | 1747μs | 6318μs | 106ms | **114x** |
| **50%匹配** | **2621μs** | 9267μs | 6064μs | 3544μs | 68ms | **26x** |
| **10%匹配** | **1235μs** | 7880μs | 3728μs | 3285μs | 29ms | **23x** |

**发现**:
- v3 仍然最优
- GPU-UMA 在 50% 匹配场景接近 v3 (74%)
- v5/v6 比 v3 慢 3-8x

### 2.3 大规模测试 (1M×1M)

| 测试场景 | v3 时间 | v5 两阶段 | v6 链式 | GPU-UMA | DuckDB | v3 vs DuckDB |
|---------|---------|-----------|---------|---------|--------|--------------|
| **全匹配** | **2611μs** | 26704μs | 11068μs | 7128μs | 205ms | **78x** |
| **10%匹配** | **2801μs** | 15047μs | 4667μs | 3564μs | 81ms | **29x** |

**发现**:
- v3 仍然是最快的 CPU 实现
- GPU-UMA 在 10% 匹配场景达到 v3 的 79%
- v6 链式在 10% 匹配场景达到 v3 的 60%

---

## 三、性能分析

### 3.1 各版本加速比 (相对于 v3)

| 版本 | 10K×100K | 100K×1M | 1M×1M | 平均 |
|------|----------|---------|-------|------|
| **v3 (基准)** | 1.00x | 1.00x | 1.00x | 1.00x |
| **v5 两阶段** | 0.07x | 0.18x | 0.10x | **0.12x** |
| **v5 并行** | 0.07x | 0.15x | 0.07x | **0.10x** |
| **v6 链式** | 0.17x | 0.53x | 0.24x | **0.31x** |
| **v6 分块** | 0.29x | 0.53x | 0.25x | **0.36x** |
| **GPU-UMA** | N/A | 0.38x | 0.37x | **0.38x** |

### 3.2 vs DuckDB 加速比

| 数据规模 | 最佳版本 | vs DuckDB |
|---------|---------|-----------|
| 10K×100K 全匹配 | v3 | **62x** |
| 10K×100K 10%匹配 | v3 | **43x** |
| 100K×1M 全匹配 | v3 | **114x** |
| 100K×1M 10%匹配 | v3 | **23x** |
| 1M×1M 全匹配 | v3 | **78x** |
| 1M×1M 10%匹配 | v3 | **29x** |

---

## 四、瓶颈分析

### 4.1 v5 两阶段 - 为什么慢？

**设计目标**: 消除 `grow_join_result()` 的 O(n) memcpy

**实际问题**:
1. **两次哈希表遍历**: Phase 1 计数 + Phase 2 填充 = 2x 内存访问
2. **冷缓存**: Phase 2 开始时哈希表已被 Phase 1 污染
3. **同步开销**: 两阶段之间的内存分配延迟

**优化建议**:
- 使用预测性容量分配 (1.5x 预期匹配数)
- 单遍历 + 动态增长 (当前 v3 策略)

### 4.2 v5 并行 - 为什么比单线程更慢？

**设计目标**: 4 线程并行分区探测

**实际问题**:
1. **分区开销**: 额外的分区阶段消耗时间
2. **伪共享**: 多线程写入结果数组导致缓存行抖动
3. **同步等待**: 线程结果合并的同步开销
4. **数据量不足**: 100K-1M 数据无法填满 4 核心流水线

**优化建议**:
- 提高并行阈值 (>10M 数据)
- 使用线程本地结果缓冲区
- 原子递增收集避免伪共享

### 4.3 v6 链式哈希 - 接近但不如 v3

**设计目标**: 链式存储所有重复键，单次查找获取所有匹配

**实际问题**:
1. **指针追踪**: 链表遍历破坏缓存预取
2. **内存碎片**: Entry 结构比 v3 的 SOA 布局占用更多内存
3. **哈希冲突**: 开放地址法在高负载下比链式更高效

**优化建议**:
- 使用 SoA 布局替代 AoS Entry
- 实现 SIMD 批量链表遍历
- 考虑 Robin Hood 探测替代链式

### 4.4 GPU-UMA - 潜力未充分发挥

**当前表现**: v3 的 38-79%

**问题分析**:
1. **Kernel 启动开销**: 1M 数据不足以分摊 ~10μs 启动成本
2. **原子操作争用**: 全局原子计数器成为瓶颈
3. **内存绑定**: GPU ALU 利用率低

**优化方向**:
- 批量原子 (SIMD prefix sum)
- 两阶段输出 (count + scatter)
- 更大数据规模测试 (10M+)

---

## 五、优化优先级

| 优先级 | 目标 | 方法 | 预期收益 |
|--------|------|------|---------|
| **P0** | 修复 v5 两阶段 | 改为预测性容量 + 单遍历 | 与 v3 持平 |
| **P1** | 优化 v5 并行 | 提高阈值 + 消除伪共享 | 大数据 1.5x |
| **P2** | 优化 v6 链式 | SoA 布局 + SIMD 批量 | v3 的 80% |
| **P3** | GPU 大规模测试 | 10M+ 数据规模 | GPU 优势显现 |

---

## 六、结论

### 6.1 核心发现

1. **v3 (CPU Neon SIMD) 是当前最优实现**
   - 在所有测试规模和匹配率下均为最快
   - 比 DuckDB 基线快 23-114x

2. **v5/v6 优化方向正确，但实现有缺陷**
   - 两阶段算法增加了内存访问次数
   - 并行版本在中等规模数据上有负收益
   - 链式哈希表的指针追踪破坏缓存

3. **GPU-UMA 在大规模数据上有潜力**
   - 1M×1M 10% 匹配达到 v3 的 79%
   - 需要更大数据规模 (10M+) 才能体现优势

### 6.2 下一步计划

1. **短期**: 修复 v5/v6 实现问题，争取与 v3 持平
2. **中期**: 实现 10M+ 规模的 GPU Hash Join 测试
3. **长期**: 研究 M4 NPU 在 Join 预过滤中的应用

---

*报告生成: ThunderDuck Benchmark Suite*
*测试时间: 2026-01-26*
